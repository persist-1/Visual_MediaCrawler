# Visual_MediaCrawler

本项目是一个前后端分离的媒体数据采集平台，能够异步、高效、直观地采集国内主流平台的媒体数据，并将其存储在本地数据库中。用户可以根据需求筛选和导出数据，以便进行数据分析和提供给BI工具使用。

本项目基于知名的开源项目 "MediaCrawler" 进行了大幅度改动，主要改进内容如下：

1. **高性能API服务**：使用FastAPI构建了支持异步高并发的API服务器，所有功能均可通过API接口进行调用，提升了系统的可扩展性和集成能力。
2. **灵活的数据库支持**：增加了SQLite数据库作为默认存储数据库，并编写了相应的数据库事务脚本，以兼容MySQL存储。同时，这种设计也更利于对其他关系型数据库的适配，为用户提供了更多数据存储选择。
3. **直观的用户界面**：构建了前端服务，提供了"数据爬取"和"数据展示"两个核心界面，使得操作更加便捷直观，充分对齐了项目的核心功能，提升了用户体验。
4. **现代化开发环境**：使用`uv`管理项目环境，并将Python版本升级到3.13.5。同时，同步升级了所有依赖包以适配新版本，并对过时的代码进行了更新，确保了项目的稳定性和前瞻性。
5. **增强的命令行与功能**：对命令行工具及其功能进行了更新，增加了以下新参数：
   - `--max_count`：用于控制单次爬取的上限条数。
   - `--task_id`：引入了"任务(每次)ID"的概念，以"每次任务"为数据管理理念，并适配了API服务，方便用户追踪和管理每次爬取任务。
   - `--storage_type`：选择数据存储类型（sqlite或mysql），支持灵活的数据库存储选择。

## 支持的平台

本项目支持以下主流媒体平台的数据采集：

- **小红书 (xhs)**：支持搜索、详情页、创作者数据采集
- **抖音 (dy)**：支持视频搜索、详情页、创作者数据采集
- **快手 (ks)**：支持视频搜索、详情页、创作者数据采集
- **哔哩哔哩 (bili)**：支持视频搜索、详情页、UP主数据采集
- **微博 (wb)**：支持微博搜索、详情页、用户数据采集
- **百度贴吧 (tieba)**：支持帖子搜索、详情页数据采集
- **知乎 (zhihu)**：支持问答搜索、详情页、用户数据采集

## 登录方式

- **二维码登录 (qrcode)**：通过扫描二维码进行登录
- **手机号登录 (phone)**：使用手机号和验证码登录
- **Cookie登录 (cookie)**：使用已有的Cookie信息登录

## 数据存储

- **SQLite数据库**：默认存储方式，轻量级本地数据库
- **MySQL数据库**：可选的数据同步方式，支持企业级数据存储
- **CSV/JSON导出**：支持将数据导出为常见的数据格式，便于数据分析

## 快速开始

### 环境要求

- Python 3.13.5+
- Node.js 16+
- uv (Python包管理器)

### 安装依赖

```bash
# 安装Python依赖
uv sync

# 安装前端依赖
cd frontend
npm install
```

### 数据库初始化

首次使用需要初始化数据库：

```bash
python db_init.py
```

### 启动服务

```bash
# 启动后端API服务
cd api
python api.py
# 或使用批处理脚本
api_run.bat

# 启动前端开发服务
cd frontend
npm run dev
# 或使用批处理脚本
frontend_run_dev.bat
```

### 访问服务

- 前端界面：http://localhost:10002/
- API文档：http://localhost:10001/docs

## 项目结构

```
Visual_MediaCrawler/
├── api/                     # FastAPI应用主文件和API相关逻辑
├── base/                    # 爬虫基础模块
├── cache/                   # 缓存模块
├── cmd_arg/                 # 命令行参数处理
├── config/                  # 项目配置
├── constant/                # 常量定义
├── debug_tools/             # 调试工具
├── dp_op/                   # 数据库操作模块
├── frontend/                # 前端服务代码
├── libs/                    # 第三方库或自定义工具库
├── media_platform/          # 各媒体平台爬虫实现
├── model/                   # 数据模型定义
├── proxy/                   # 代理相关配置和实现
├── schema/                  # 数据库表结构定义
├── store/                   # 数据存储逻辑
├── test/                    # 测试文件
├── tools/                   # 通用工具函数
└── .docs/                   # 项目文档
```

## 后端API接口

### 爬虫任务管理接口

- **GET /**：API根路径，返回服务状态信息
- **POST /crawler/run**：同步执行爬虫任务，等待任务完成后返回结果
- **POST /crawler/run-async**：异步执行爬虫任务，立即返回任务ID，任务在后台执行
- **GET /crawler/task/{task_times_id}**：查询指定任务的执行状态和结果
- **GET /crawler/tasks**：获取所有任务的列表和状态信息
- **DELETE /crawler/task/{task_times_id}**：删除指定任务的记录
- **GET /health**：健康检查接口，用于监控API服务状态

### 数据管理接口

- **GET /sqlite/tables**：获取所有可用的SQLite数据表列表
- **GET /sqlite/data**：获取指定SQLite表格的数据，支持分页和任务ID筛选
- **GET /sqlite/stats**：获取SQLite数据库的统计信息
- **GET /sqlite/export**：导出指定SQLite表格的数据为CSV格式
- **GET /sqlite/export-json**：导出指定SQLite表格的数据为JSON格式
- **GET /sqlite/configs**：获取所有数据表的配置信息

## 详细文档

详细文档请参考：

- [项目环境管理指南](./.docs/项目环境管理指南.md) - 前后端环境配置与管理
- [项目代码结构](./.docs/项目代码结构.md) - 项目结构详细说明
- [数据库初始化指南](./.docs/数据库初始化指南.md) - 数据库配置与初始化

## 技术特性

- **异步高并发**：基于FastAPI和asyncio实现高性能数据采集
- **模块化设计**：清晰的项目结构，易于维护和扩展
- **多平台支持**：覆盖国内主流媒体平台
- **灵活的数据存储**：支持SQLite和MySQL双重存储
- **任务管理**：完整的任务生命周期管理
- **数据导出**：多种格式的数据导出功能
- **前端可视化**：直观的Web界面操作

## 许可证

本项目基于开源许可证发布，仅供学习和研究目的使用。使用时请遵守相关平台的使用条款和robots.txt规则，合理控制请求频率，避免对目标平台造成不必要的负担。

## 贡献

欢迎提交Issue和Pull Request来帮助改进项目。在贡献代码前，请确保遵循项目的代码规范和最佳实践。
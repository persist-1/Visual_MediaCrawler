#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ•°æ®åº“åˆ†æå·¥å…·
ç”¨äºåˆ†æSQLiteæ•°æ®åº“ç»“æ„å¹¶å¯¼å‡ºä¸ºSQLæ–‡ä»¶å’Œæ•°æ®æ¨¡å‹æ–‡ä»¶
"""

import sqlite3
import os
from datetime import datetime
from typing import List, Dict, Any, Tuple
import json


class SQLiteAnalyzer:
    """SQLiteæ•°æ®åº“åˆ†æå™¨"""
    
    def __init__(self, db_path: str):
        """
        åˆå§‹åŒ–åˆ†æå™¨
        
        Args:
            db_path: SQLiteæ•°æ®åº“æ–‡ä»¶è·¯å¾„
        """
        self.db_path = db_path
        self.conn = None
        
    def connect(self):
        """è¿æ¥æ•°æ®åº“"""
        try:
            self.conn = sqlite3.connect(self.db_path)
            self.conn.row_factory = sqlite3.Row
            print(f"âœ… æˆåŠŸè¿æ¥åˆ°æ•°æ®åº“: {self.db_path}")
        except Exception as e:
            print(f"âŒ è¿æ¥æ•°æ®åº“å¤±è´¥: {e}")
            raise
    
    def close(self):
        """å…³é—­æ•°æ®åº“è¿æ¥"""
        if self.conn:
            self.conn.close()
            print("âœ… æ•°æ®åº“è¿æ¥å·²å…³é—­")
    
    def get_all_tables(self) -> List[str]:
        """è·å–æ‰€æœ‰è¡¨å"""
        cursor = self.conn.cursor()
        cursor.execute("""
            SELECT name FROM sqlite_master 
            WHERE type='table' AND name NOT LIKE 'sqlite_%'
            ORDER BY name
        """)
        tables = [row[0] for row in cursor.fetchall()]
        print(f"ğŸ“Š å‘ç° {len(tables)} ä¸ªæ•°æ®è¡¨")
        return tables
    
    def get_table_info(self, table_name: str) -> List[Dict[str, Any]]:
        """è·å–è¡¨ç»“æ„ä¿¡æ¯"""
        cursor = self.conn.cursor()
        cursor.execute(f"PRAGMA table_info({table_name})")
        columns = []
        for row in cursor.fetchall():
            columns.append({
                'cid': row[0],
                'name': row[1],
                'type': row[2],
                'notnull': bool(row[3]),
                'default_value': row[4],
                'pk': bool(row[5])
            })
        return columns
    
    def get_table_indexes(self, table_name: str) -> List[Dict[str, Any]]:
        """è·å–è¡¨ç´¢å¼•ä¿¡æ¯"""
        cursor = self.conn.cursor()
        cursor.execute(f"PRAGMA index_list({table_name})")
        indexes = []
        for row in cursor.fetchall():
            index_name = row[1]
            # è·å–ç´¢å¼•è¯¦ç»†ä¿¡æ¯
            cursor.execute(f"PRAGMA index_info({index_name})")
            index_columns = [col[2] for col in cursor.fetchall()]
            indexes.append({
                'name': index_name,
                'unique': bool(row[2]),
                'columns': index_columns
            })
        return indexes
    
    def get_table_create_sql(self, table_name: str) -> str:
        """è·å–è¡¨çš„åˆ›å»ºSQLè¯­å¥"""
        cursor = self.conn.cursor()
        cursor.execute("""
            SELECT sql FROM sqlite_master 
            WHERE type='table' AND name=?
        """, (table_name,))
        result = cursor.fetchone()
        return result[0] if result else ""
    
    def _enhance_create_sql(self, original_sql: str, columns: List[Dict[str, Any]]) -> str:
        """å¢å¼ºCREATE TABLEè¯­å¥ï¼Œç¡®ä¿åŒ…å«task_times_idå­—æ®µ"""
        # æ£€æŸ¥æ˜¯å¦å·²æœ‰task_times_idå­—æ®µ
        has_task_times_id = any(col['name'] == 'task_times_id' for col in columns)
        
        if has_task_times_id:
            return original_sql
        
        # å¦‚æœæ²¡æœ‰task_times_idå­—æ®µï¼Œæ·»åŠ å®ƒ
        # æ‰¾åˆ°æœ€åä¸€ä¸ªå­—æ®µçš„ä½ç½®
        if original_sql.endswith(');'):
            # åœ¨æœ€åä¸€ä¸ªå­—æ®µåæ·»åŠ task_times_id
            enhanced_sql = original_sql[:-2] + ',\n    `task_times_id` TEXT DEFAULT NULL\n);'
            return enhanced_sql
        elif original_sql.endswith(')'):
            enhanced_sql = original_sql[:-1] + ',\n    `task_times_id` TEXT DEFAULT NULL\n)'
            return enhanced_sql
        else:
            return original_sql
    
    def get_table_row_count(self, table_name: str) -> int:
        """è·å–è¡¨çš„è¡Œæ•°"""
        cursor = self.conn.cursor()
        cursor.execute(f"SELECT COUNT(*) FROM {table_name}")
        return cursor.fetchone()[0]
    
    def analyze_database(self) -> Dict[str, Any]:
        """åˆ†ææ•´ä¸ªæ•°æ®åº“"""
        print("ğŸ” å¼€å§‹åˆ†ææ•°æ®åº“...")
        
        analysis = {
            'database_path': self.db_path,
            'analysis_time': datetime.now().isoformat(),
            'tables': {}
        }
        
        tables = self.get_all_tables()
        
        for table_name in tables:
            print(f"ğŸ“‹ åˆ†æè¡¨: {table_name}")
            
            columns = self.get_table_info(table_name)
            indexes = self.get_table_indexes(table_name)
            create_sql = self.get_table_create_sql(table_name)
            row_count = self.get_table_row_count(table_name)
            
            # æ£€æŸ¥å¹¶æ·»åŠ task_times_idå­—æ®µä¿¡æ¯
            enhanced_columns = self._ensure_task_times_id_column(columns)
            
            analysis['tables'][table_name] = {
                'columns': enhanced_columns,
                'indexes': indexes,
                'create_sql': create_sql,
                'row_count': row_count
            }
        
        print(f"âœ… æ•°æ®åº“åˆ†æå®Œæˆï¼Œå…±åˆ†æ {len(tables)} ä¸ªè¡¨")
        return analysis
    
    def export_to_sql(self, output_path: str, analysis: Dict[str, Any]):
        """å¯¼å‡ºä¸ºSQLæ–‡ä»¶"""
        print(f"ğŸ“ å¯¼å‡ºSQLæ–‡ä»¶åˆ°: {output_path}")
        
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(f"-- SQLiteæ•°æ®åº“ç»“æ„å¯¼å‡º\n")
            f.write(f"-- æ•°æ®åº“æ–‡ä»¶: {analysis['database_path']}\n")
            f.write(f"-- å¯¼å‡ºæ—¶é—´: {analysis['analysis_time']}\n")
            f.write(f"-- ç”Ÿæˆå·¥å…·: MediaCrawler DB Analyzer\n\n")
            
            for table_name, table_info in analysis['tables'].items():
                f.write(f"-- ----------------------------\n")
                f.write(f"-- Table structure for {table_name}\n")
                f.write(f"-- ----------------------------\n")
                f.write(f"DROP TABLE IF EXISTS `{table_name}`;\n")
                
                # é‡æ–°æ„å»ºCREATE TABLEè¯­å¥ï¼Œç¡®ä¿åŒ…å«task_times_idå­—æ®µ
                enhanced_sql = self._enhance_create_sql(table_info['create_sql'], table_info['columns'])
                f.write(f"{enhanced_sql};\n\n")
                
                # ç”Ÿæˆå®é™…çš„CREATE INDEXè¯­å¥
                if table_info['indexes']:
                    for index in table_info['indexes']:
                        if not index['name'].startswith('sqlite_autoindex'):
                            columns_str = '`, `'.join(index['columns'])
                            if index['unique']:
                                f.write(f"CREATE UNIQUE INDEX `{index['name']}` ON `{table_name}` (`{columns_str}`);\n")
                            else:
                                f.write(f"CREATE INDEX `{index['name']}` ON `{table_name}` (`{columns_str}`);\n")
                    
                    # ä¸ºtask_times_idå­—æ®µæ·»åŠ ç´¢å¼•ï¼ˆå¦‚æœå­˜åœ¨ä¸”æ²¡æœ‰ç´¢å¼•ï¼‰
                    has_task_times_id = any(col['name'] == 'task_times_id' for col in table_info['columns'])
                    has_task_times_id_index = any('task_times_id' in index['columns'] for index in table_info['indexes'])
                    
                    if has_task_times_id and not has_task_times_id_index:
                        f.write(f"CREATE INDEX `idx_{table_name}_task_times_id` ON `{table_name}` (`task_times_id`);\n")
                    
                    f.write("\n")
        
        print("âœ… SQLæ–‡ä»¶å¯¼å‡ºå®Œæˆ")
    
    def export_to_datamodel(self, output_path: str, analysis: Dict[str, Any]):
        """å¯¼å‡ºä¸ºPythonæ•°æ®æ¨¡å‹æ–‡ä»¶"""
        print(f"ğŸ å¯¼å‡ºæ•°æ®æ¨¡å‹æ–‡ä»¶åˆ°: {output_path}")
        
        # SQLiteåˆ°Pythonç±»å‹æ˜ å°„
        type_mapping = {
            'INTEGER': 'int',
            'TEXT': 'str',
            'REAL': 'float',
            'BLOB': 'bytes',
            'NUMERIC': 'float',
            'VARCHAR': 'str',
            'CHAR': 'str',
            'DATETIME': 'datetime',
            'TIMESTAMP': 'datetime',
            'DATE': 'date',
            'TIME': 'time',
            'BOOLEAN': 'bool',
            'BOOL': 'bool'
        }
        
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write('#!/usr/bin/env python3\n')
            f.write('# -*- coding: utf-8 -*-\n')
            f.write('"""\n')
            f.write('æ•°æ®æ¨¡å‹å®šä¹‰\n')
            f.write(f'ä»SQLiteæ•°æ®åº“è‡ªåŠ¨ç”Ÿæˆ: {analysis["database_path"]}\n')
            f.write(f'ç”Ÿæˆæ—¶é—´: {analysis["analysis_time"]}\n')
            f.write('ç”Ÿæˆå·¥å…·: MediaCrawler DB Analyzer\n')
            f.write('"""\n\n')
            
            f.write('from datetime import datetime, date, time\n')
            f.write('from typing import Optional, List\n')
            f.write('from dataclasses import dataclass\n')
            f.write('from sqlalchemy import Column, Integer, String, Text, DateTime, Boolean, Float\n')
            f.write('from sqlalchemy.ext.declarative import declarative_base\n\n')
            
            f.write('Base = declarative_base()\n\n')
            
            # ç”Ÿæˆæ•°æ®ç±»
            for table_name, table_info in analysis['tables'].items():
                class_name = self._table_name_to_class_name(table_name)
                
                f.write(f'@dataclass\n')
                f.write(f'class {class_name}Data:\n')
                f.write(f'    """æ•°æ®ç±»: {table_name} (è¡Œæ•°: {table_info["row_count"]})"""\n')
                
                for col in table_info['columns']:
                    python_type = self._sqlite_type_to_python(col['type'], type_mapping)
                    optional = '' if col['notnull'] or col['pk'] else 'Optional['
                    optional_end = '' if col['notnull'] or col['pk'] else ']'
                    default = ' = None' if not col['notnull'] and not col['pk'] else ''
                    
                    f.write(f'    {col["name"]}: {optional}{python_type}{optional_end}{default}\n')
                
                f.write('\n')
                
                # ç”ŸæˆSQLAlchemyæ¨¡å‹
                f.write(f'class {class_name}(Base):\n')
                f.write(f'    """SQLAlchemyæ¨¡å‹: {table_name}"""\n')
                f.write(f'    __tablename__ = \'{table_name}\'\n\n')
                
                for col in table_info['columns']:
                    sqlalchemy_type = self._sqlite_type_to_sqlalchemy(col['type'])
                    primary_key = ', primary_key=True' if col['pk'] else ''
                    nullable = ', nullable=False' if col['notnull'] else ''
                    
                    f.write(f'    {col["name"]} = Column({sqlalchemy_type}{primary_key}{nullable})\n')
                
                f.write('\n')
                
                # æ·»åŠ ç´¢å¼•ä¿¡æ¯ä½œä¸ºæ³¨é‡Š
                if table_info['indexes']:
                    f.write(f'    # ç´¢å¼•ä¿¡æ¯:\n')
                    for index in table_info['indexes']:
                        if not index['name'].startswith('sqlite_autoindex'):
                            index_type = 'UNIQUE' if index['unique'] else 'INDEX'
                            f.write(f'    # - {index["name"]}: {", ".join(index["columns"])} ({index_type})\n')
                
                f.write('\n\n')
        
        print("âœ… æ•°æ®æ¨¡å‹æ–‡ä»¶å¯¼å‡ºå®Œæˆ")
    
    def _table_name_to_class_name(self, table_name: str) -> str:
        """å°†è¡¨åè½¬æ¢ä¸ºç±»å"""
        # ç§»é™¤å‰ç¼€ï¼Œè½¬æ¢ä¸ºé©¼å³°å‘½å
        parts = table_name.split('_')
        return ''.join(word.capitalize() for word in parts)
    
    def _sqlite_type_to_python(self, sqlite_type: str, type_mapping: Dict[str, str]) -> str:
        """å°†SQLiteç±»å‹è½¬æ¢ä¸ºPythonç±»å‹"""
        sqlite_type = sqlite_type.upper()
        for sql_type, py_type in type_mapping.items():
            if sql_type in sqlite_type:
                return py_type
        return 'str'  # é»˜è®¤ç±»å‹
    
    def _sqlite_type_to_sqlalchemy(self, sqlite_type: str) -> str:
        """å°†SQLiteç±»å‹è½¬æ¢ä¸ºSQLAlchemyç±»å‹"""
        sqlite_type = sqlite_type.upper()
        if 'INTEGER' in sqlite_type:
            return 'Integer'
        elif 'TEXT' in sqlite_type or 'VARCHAR' in sqlite_type or 'CHAR' in sqlite_type:
            return 'Text'
        elif 'REAL' in sqlite_type or 'FLOAT' in sqlite_type or 'NUMERIC' in sqlite_type:
            return 'Float'
        elif 'DATETIME' in sqlite_type or 'TIMESTAMP' in sqlite_type:
            return 'DateTime'
        elif 'BOOL' in sqlite_type:
            return 'Boolean'
        else:
            return 'String'
    
    def _ensure_task_times_id_column(self, columns: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """ç¡®ä¿åˆ—ä¿¡æ¯ä¸­åŒ…å«task_times_idå­—æ®µ"""
        # æ£€æŸ¥æ˜¯å¦å·²æœ‰task_times_idå­—æ®µ
        has_task_times_id = any(col['name'] == 'task_times_id' for col in columns)
        
        if not has_task_times_id:
            # æ·»åŠ task_times_idå­—æ®µ
            task_times_id_col = {
                'cid': len(columns),
                'name': 'task_times_id',
                'type': 'TEXT',
                'notnull': False,
                'default_value': None,
                'pk': False
            }
            columns.append(task_times_id_col)
        
        return columns
    
    def export_analysis_json(self, output_path: str, analysis: Dict[str, Any]):
        """å¯¼å‡ºåˆ†æç»“æœä¸ºJSONæ–‡ä»¶"""
        print(f"ğŸ“„ å¯¼å‡ºåˆ†æç»“æœåˆ°: {output_path}")
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(analysis, f, ensure_ascii=False, indent=2)
        
        print("âœ… JSONæ–‡ä»¶å¯¼å‡ºå®Œæˆ")


def main():
    """ä¸»å‡½æ•°"""
    # æ•°æ®åº“è·¯å¾„
    db_path = "d:/A_work/A_trae_alter/MediaCrawler-main/data/mc.db"
    
    # è¾“å‡ºç›®å½•
    output_dir = "./output"
    os.makedirs(output_dir, exist_ok=True)
    
    # åˆ›å»ºåˆ†æå™¨
    analyzer = SQLiteAnalyzer(db_path)
    
    try:
        # è¿æ¥æ•°æ®åº“
        analyzer.connect()
        
        # åˆ†ææ•°æ®åº“
        analysis = analyzer.analyze_database()
        
        # ç”Ÿæˆæ—¶é—´æˆ³
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # å¯¼å‡ºæ–‡ä»¶
        sql_file = os.path.join(output_dir, f"database_structure_{timestamp}.sql")
        model_file = os.path.join(output_dir, f"datamodel_{timestamp}.py")
        json_file = os.path.join(output_dir, f"analysis_{timestamp}.json")
        
        analyzer.export_to_sql(sql_file, analysis)
        analyzer.export_to_datamodel(model_file, analysis)
        analyzer.export_analysis_json(json_file, analysis)
        
        print("\nğŸ‰ æ‰€æœ‰æ–‡ä»¶å¯¼å‡ºå®Œæˆ!")
        print(f"ğŸ“ è¾“å‡ºç›®å½•: {os.path.abspath(output_dir)}")
        print(f"ğŸ“„ SQLæ–‡ä»¶: {sql_file}")
        print(f"ğŸ æ•°æ®æ¨¡å‹: {model_file}")
        print(f"ğŸ“Š åˆ†æç»“æœ: {json_file}")
        
    except Exception as e:
        print(f"âŒ æ‰§è¡Œå¤±è´¥: {e}")
    finally:
        analyzer.close()


if __name__ == "__main__":
    main()
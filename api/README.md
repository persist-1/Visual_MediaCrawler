# MediaCrawler API æœåŠ¡

è¿™æ˜¯ä¸€ä¸ªåŸºäº FastAPI çš„ MediaCrawler API æœåŠ¡ï¼Œæä¾›äº†ä¸å‘½ä»¤è¡Œå·¥å…·ç›¸åŒçš„åŠŸèƒ½ï¼Œä½†é€šè¿‡ HTTP API æ¥å£è¿›è¡Œè°ƒç”¨ã€‚

## ğŸš€ å¿«é€Ÿå¼€å§‹

### å®‰è£…ä¾èµ–

```bash
# è¿›å…¥ api ç›®å½•
cd api

```

### å¯åŠ¨æœåŠ¡

```bash
# ä½¿ç”¨ uvicorn å¯åŠ¨
uvicorn api:app --host 0.0.0.0 --port 10001 --reload
```



## ğŸ“‹ API æ¥å£è¯´æ˜

### 1. åŒæ­¥æ‰§è¡Œçˆ¬è™«ä»»åŠ¡

**æ¥å£**: `POST /crawler/run`

**æè¿°**: åŒæ­¥æ‰§è¡Œçˆ¬è™«ä»»åŠ¡ï¼Œç­‰å¾…ä»»åŠ¡å®Œæˆåè¿”å›ç»“æœ

**è¯·æ±‚ç¤ºä¾‹**:
```json
{
  "platform": "xhs",
  "lt": "qrcode",
  "type": "detail",
  "keywords": "ç¼–ç¨‹å‰¯ä¸š",
  "get_comment": true,
  "save_data_option": "json"
}
```

### 2. å¼‚æ­¥æ‰§è¡Œçˆ¬è™«ä»»åŠ¡

**æ¥å£**: `POST /crawler/run-async`

**æè¿°**: å¼‚æ­¥æ‰§è¡Œçˆ¬è™«ä»»åŠ¡ï¼Œç«‹å³è¿”å›ä»»åŠ¡IDï¼Œä»»åŠ¡åœ¨åå°æ‰§è¡Œ

**è¯·æ±‚ç¤ºä¾‹**:
```json
{
  "platform": "xhs",
  "lt": "qrcode",
  "type": "search",
  "keywords": "ç¼–ç¨‹å…¼èŒ",
  "get_comment": false,
  "save_data_option": "csv"
}
```

**å“åº”ç¤ºä¾‹**:
```json
{
  "success": true,
  "message": "çˆ¬è™«ä»»åŠ¡å·²æäº¤ï¼Œæ­£åœ¨åå°æ‰§è¡Œ",
  "task_id": "550e8400-e29b-41d4-a716-446655440000"
}
```

### 3. æŸ¥è¯¢ä»»åŠ¡çŠ¶æ€

**æ¥å£**: `GET /crawler/task/{task_id}`

**æè¿°**: æŸ¥è¯¢å¼‚æ­¥ä»»åŠ¡çš„æ‰§è¡ŒçŠ¶æ€

**å“åº”ç¤ºä¾‹**:
```json
{
  "task_id": "550e8400-e29b-41d4-a716-446655440000",
  "status": "completed",
  "message": "ä»»åŠ¡æ‰§è¡ŒæˆåŠŸ",
  "result": {
    "stdout": "çˆ¬è™«æ‰§è¡Œæ—¥å¿—...",
    "stderr": ""
  }
}
```

### 4. è·å–æ‰€æœ‰ä»»åŠ¡åˆ—è¡¨

**æ¥å£**: `GET /crawler/tasks`

**æè¿°**: è·å–æ‰€æœ‰ä»»åŠ¡çš„åˆ—è¡¨å’ŒçŠ¶æ€

### 5. åˆ é™¤ä»»åŠ¡è®°å½•

**æ¥å£**: `DELETE /crawler/task/{task_id}`

**æè¿°**: åˆ é™¤æŒ‡å®šçš„ä»»åŠ¡è®°å½•

## ğŸ“ è¯·æ±‚å‚æ•°è¯´æ˜

| å‚æ•°å | ç±»å‹ | å¿…å¡« | é»˜è®¤å€¼ | è¯´æ˜ |
|--------|------|------|--------|------|
| platform | string | å¦ | "xhs" | åª’ä½“å¹³å°é€‰æ‹© (xhs\|dy\|ks\|bili\|wb\|tieba\|zhihu) |
| lt | string | å¦ | "qrcode" | ç™»å½•ç±»å‹ (qrcode\|phone\|cookie) |
| type | string | å¦ | "search" | çˆ¬è™«ç±»å‹ (search\|detail\|creator) |
| start | integer | å¦ | null | èµ·å§‹é¡µæ•° |
| keywords | string | å¦ | null | æœç´¢å…³é”®è¯ |
| get_comment | boolean | å¦ | null | æ˜¯å¦çˆ¬å–ä¸€çº§è¯„è®º |
| get_sub_comment | boolean | å¦ | null | æ˜¯å¦çˆ¬å–äºŒçº§è¯„è®º |
| save_data_option | string | å¦ | "json" | æ•°æ®ä¿å­˜æ–¹å¼ (csv\|db\|json) |
| cookies | string | å¦ | null | ç”¨äºcookieç™»å½•ç±»å‹çš„cookies |

## ğŸ”§ ä½¿ç”¨ç¤ºä¾‹

### Python å®¢æˆ·ç«¯ç¤ºä¾‹

```python
import requests
import time

# API åŸºç¡€åœ°å€
base_url = "http://localhost:10001"

# 1. åŒæ­¥æ‰§è¡Œçˆ¬è™«ä»»åŠ¡
def sync_crawl():
    data = {
        "platform": "xhs",
        "lt": "qrcode",
        "type": "detail",
        "keywords": "ç¼–ç¨‹å‰¯ä¸š",
        "get_comment": True,
        "save_data_option": "json"
    }
    
    response = requests.post(f"{base_url}/crawler/run", json=data)
    result = response.json()
    
    if result["success"]:
        print("çˆ¬è™«ä»»åŠ¡æ‰§è¡ŒæˆåŠŸ")
        print(result["data"]["stdout"])
    else:
        print(f"çˆ¬è™«ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {result['message']}")

# 2. å¼‚æ­¥æ‰§è¡Œçˆ¬è™«ä»»åŠ¡
def async_crawl():
    data = {
        "platform": "xhs",
        "lt": "qrcode",
        "type": "search",
        "keywords": "ç¼–ç¨‹å…¼èŒ",
        "get_comment": False,
        "save_data_option": "csv"
    }
    
    # æäº¤ä»»åŠ¡
    response = requests.post(f"{base_url}/crawler/run-async", json=data)
    result = response.json()
    
    if result["success"]:
        task_id = result["task_id"]
        print(f"ä»»åŠ¡å·²æäº¤ï¼Œä»»åŠ¡ID: {task_id}")
        
        # è½®è¯¢ä»»åŠ¡çŠ¶æ€
        while True:
            status_response = requests.get(f"{base_url}/crawler/task/{task_id}")
            status_result = status_response.json()
            
            print(f"ä»»åŠ¡çŠ¶æ€: {status_result['status']} - {status_result['message']}")
            
            if status_result["status"] in ["completed", "failed"]:
                if status_result["result"]:
                    print("æ‰§è¡Œç»“æœ:")
                    print(status_result["result"]["stdout"])
                break
            
            time.sleep(5)  # ç­‰å¾…5ç§’åå†æ¬¡æŸ¥è¯¢
    else:
        print(f"ä»»åŠ¡æäº¤å¤±è´¥: {result['message']}")

if __name__ == "__main__":
    # é€‰æ‹©æ‰§è¡Œæ–¹å¼
    # sync_crawl()  # åŒæ­¥æ‰§è¡Œ
    async_crawl()   # å¼‚æ­¥æ‰§è¡Œ
```

### cURL ç¤ºä¾‹

```bash
# 1. å¥åº·æ£€æŸ¥
curl -X GET "http://localhost:10001/health"

# 2. åŒæ­¥æ‰§è¡Œçˆ¬è™«ä»»åŠ¡
curl -X POST "http://localhost:10001/crawler/run" \
     -H "Content-Type: application/json" \
     -d '{
       "platform": "xhs",
       "lt": "qrcode",
       "type": "detail",
       "keywords": "ç¼–ç¨‹å‰¯ä¸š",
       "get_comment": true,
       "save_data_option": "json"
     }'

# 3. å¼‚æ­¥æ‰§è¡Œçˆ¬è™«ä»»åŠ¡
curl -X POST "http://localhost:10001/crawler/run-async" \
     -H "Content-Type: application/json" \
     -d '{
       "platform": "xhs",
       "lt": "qrcode",
       "type": "search",
       "keywords": "ç¼–ç¨‹å…¼èŒ",
       "get_comment": false,
       "save_data_option": "csv"
     }'

# 4. æŸ¥è¯¢ä»»åŠ¡çŠ¶æ€ï¼ˆæ›¿æ¢ {task_id} ä¸ºå®é™…çš„ä»»åŠ¡IDï¼‰
curl -X GET "http://localhost:10001/crawler/task/{task_id}"

# 5. è·å–æ‰€æœ‰ä»»åŠ¡åˆ—è¡¨
curl -X GET "http://localhost:10001/crawler/tasks"
```

## ğŸ” æ³¨æ„äº‹é¡¹

1. **ç¯å¢ƒè¦æ±‚**: ç¡®ä¿å·²ç»æŒ‰ç…§ä¸»é¡¹ç›®çš„è¦æ±‚å®‰è£…äº†æ‰€æœ‰ä¾èµ–ï¼ˆuvã€playwrightç­‰ï¼‰
2. **å·¥ä½œç›®å½•**: API æœåŠ¡ä¼šåœ¨é¡¹ç›®æ ¹ç›®å½•ä¸‹æ‰§è¡Œ `uv run main.py` å‘½ä»¤
3. **ç™»å½•çŠ¶æ€**: å¦‚æœä½¿ç”¨äºŒç»´ç ç™»å½•ï¼Œéœ€è¦åœ¨å‘½ä»¤è¡Œä¸­æ‰‹åŠ¨æ‰«ç 
4. **ä»»åŠ¡ç®¡ç†**: å¼‚æ­¥ä»»åŠ¡çš„çŠ¶æ€ä¿¡æ¯å­˜å‚¨åœ¨å†…å­˜ä¸­ï¼ŒæœåŠ¡é‡å¯åä¼šä¸¢å¤±
5. **å¹¶å‘é™åˆ¶**: å»ºè®®ä¸è¦åŒæ—¶æ‰§è¡Œè¿‡å¤šçš„çˆ¬è™«ä»»åŠ¡ï¼Œä»¥å…å¯¹ç›®æ ‡å¹³å°é€ æˆå‹åŠ›

## ğŸš€ å¿«é€Ÿå¯åŠ¨

### æ–¹å¼ä¸€ï¼šä½¿ç”¨å¯åŠ¨è„šæœ¬ï¼ˆæ¨èï¼‰
```bash
# åŸºæœ¬å¯åŠ¨
python start_api.py

# è‡ªå®šä¹‰ç«¯å£å’Œåœ°å€
python start_api.py --host 0.0.0.0 --port 8080

# å¼€å‘æ¨¡å¼ï¼ˆçƒ­é‡è½½ï¼‰
python start_api.py --dev

# æŸ¥çœ‹æ‰€æœ‰é€‰é¡¹
python start_api.py --help
```

### æ–¹å¼äºŒï¼šä½¿ç”¨ Docker
```bash
# æ„å»ºå¹¶å¯åŠ¨æœåŠ¡
docker-compose up -d

# æŸ¥çœ‹æœåŠ¡çŠ¶æ€
docker-compose ps

# æŸ¥çœ‹æ—¥å¿—
docker-compose logs -f mediacrawler-api

# åœæ­¢æœåŠ¡
docker-compose down
```

### æ–¹å¼ä¸‰ï¼šä½¿ç”¨è‡ªåŠ¨åŒ–éƒ¨ç½²è„šæœ¬
```bash
# æœ¬åœ°å¼€å‘éƒ¨ç½²
python deploy.py local --start --dev

# Docker éƒ¨ç½²
python deploy.py docker --build --detach

# ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²ï¼ˆéœ€è¦ sudo æƒé™ï¼‰
python deploy.py production --port 10001 --start
```

### æ–¹å¼å››ï¼šç›´æ¥ä½¿ç”¨ uvicorn
```bash
uvicorn api:app --host 0.0.0.0 --port 10001 --reload
```

## âš™ï¸ é…ç½®è¯´æ˜

### åŸºç¡€é…ç½®
1. å¤åˆ¶é…ç½®æ–‡ä»¶æ¨¡æ¿ï¼š
```bash
cp config.example.py config.py
```

2. æ ¹æ®éœ€è¦ä¿®æ”¹ `config.py` ä¸­çš„é…ç½®é¡¹ï¼š
   - API æœåŠ¡é…ç½®ï¼ˆç«¯å£ã€æ—¥å¿—ç­‰ï¼‰
   - ä»»åŠ¡ç®¡ç†é…ç½®ï¼ˆå¹¶å‘æ•°ã€è¶…æ—¶ç­‰ï¼‰
   - å®‰å…¨é…ç½®ï¼ˆAPIå¯†é’¥ã€CORSç­‰ï¼‰
   - æ•°æ®åº“é…ç½®ï¼ˆå¦‚æœéœ€è¦æŒä¹…åŒ–ä»»åŠ¡çŠ¶æ€ï¼‰

### Docker é…ç½®
- `Dockerfile`: å®¹å™¨é•œåƒæ„å»ºé…ç½®
- `docker-compose.yml`: å®Œæ•´æœåŠ¡æ ˆé…ç½®ï¼ŒåŒ…å«ï¼š
  - MediaCrawler API æœåŠ¡
  - Redis ç¼“å­˜æœåŠ¡ï¼ˆå¯é€‰ï¼‰
  - MySQL æ•°æ®åº“æœåŠ¡ï¼ˆå¯é€‰ï¼‰



## ğŸ› ï¸ å¼€å‘è¯´æ˜

### é¡¹ç›®ç»“æ„
```
api/
â”œâ”€â”€ api.py                 # FastAPI åº”ç”¨ä¸»æ–‡ä»¶
â”œâ”€â”€ test_client.py        # æµ‹è¯•å®¢æˆ·ç«¯
â””â”€â”€ README.md            # æœ¬æ–‡æ¡£
```

### æ‰©å±•åŠŸèƒ½

- âœ… ä»»åŠ¡çŠ¶æ€ç®¡ç†å’ŒæŸ¥è¯¢
- âœ… å¼‚æ­¥ä»»åŠ¡æ‰§è¡Œ
- âœ… Docker å®¹å™¨åŒ–éƒ¨ç½²
- âœ… é…ç½®æ–‡ä»¶ç®¡ç†
- ğŸ”„ ç”¨æˆ·è®¤è¯å’Œæƒé™æ§åˆ¶
- ğŸ”„ ä»»åŠ¡é˜Ÿåˆ—å’Œåˆ†å¸ƒå¼å¤„ç†
- ğŸ”„ æ•°æ®å¯è§†åŒ–ç•Œé¢
- ğŸ”„ ç›‘æ§å’Œæ—¥å¿—ç³»ç»Ÿ
- ğŸ”„ æ›´å¤šæ•°æ®æ ¼å¼å¯¼å‡º

### æ€§èƒ½ä¼˜åŒ–å»ºè®®
1. **å¹¶å‘æ§åˆ¶**ï¼šæ ¹æ®æœåŠ¡å™¨æ€§èƒ½è°ƒæ•´ `max_concurrent_tasks`
2. **èµ„æºç›‘æ§**ï¼šå¯ç”¨ç›‘æ§åŠŸèƒ½ï¼Œè§‚å¯ŸCPUå’Œå†…å­˜ä½¿ç”¨æƒ…å†µ
3. **æ•°æ®åº“ä¼˜åŒ–**ï¼šå¦‚æœä½¿ç”¨æ•°æ®åº“å­˜å‚¨ï¼Œå»ºè®®ä½¿ç”¨ MySQL æˆ– PostgreSQL
4. **ç¼“å­˜ç­–ç•¥**ï¼šå¯ç”¨ Redis ç¼“å­˜ï¼Œæé«˜å“åº”é€Ÿåº¦
5. **è´Ÿè½½å‡è¡¡**ï¼šç”Ÿäº§ç¯å¢ƒå»ºè®®ä½¿ç”¨ Nginx è¿›è¡Œè´Ÿè½½å‡è¡¡

## ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®éµå¾ªä¸ä¸»é¡¹ç›®ç›¸åŒçš„è®¸å¯è¯æ¡æ¬¾ï¼Œä»…ä¾›å­¦ä¹ å’Œç ”ç©¶ä½¿ç”¨ã€‚